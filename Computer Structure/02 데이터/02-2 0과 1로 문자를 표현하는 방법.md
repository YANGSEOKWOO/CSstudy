컴퓨터가 문자를 이해하고 표현하는 방법에는 뭐가 있을까?

#### 용어정리
- 문자집합 : 컴퓨터가 인식하고 표현할 수 있는 문자의 모음
- 인코딩 : 문자집합을 0, 1로 변환하는 과정
- 디코딩 :0, 1로 이뤄진 문자코드를 사람이 이해하는문자로 표현 하는 과정
- 문자코드 : 인코딩의 결과값
## 문자 집합과 인코딩

컴퓨터는 문자집합에 있는 문자는 이해할 수 있지만, 문자집합에 없다면 이해할 수 없다는 것
- ex) {a, b, c, d, e} 라면, f는 이해할 수 없다.

하지만 문자를 0, 1로 표현해야지 컴퓨터가 이해할 수 있다,
- 문자집합을 0, 1로 표현하는 과정 : **인코딩** => 결과값 : 문자코드
- 0, 1로 이뤄진 문자코드를 사람이 이해하는문자로 표현 하는 과정 : **디코딩**


### 아스키코드
- 초창기 문자집합
- 알파벳, 아라비아 숫자, 일부 특수문자
- 128개 = 7비트, 패리티비트 = 1비트  => 8비트

단점 : 한글을 표현못함, 너무 작음 표현범위가

### EUC-KR : 한글 인코딩 방식
영어 : 알파벳을 쭉 이어쓰면 단어가 된다.
한글: 음절 하나하나가 초성, 중성, 종성의 조합이 된다.

결합된 한글 단어 하나에 2바이트 크기의 코드를 부여한다.

2가지 방식이 존재한다.
- 완성형 인코딩
- 조합형 인코딩

#### 완성형 인코딩
- 완성된 하나의 글자에 고유한 코드를 부여하는 인코딩 방식
- '가' : 1, '나' : 2 이런식
#### 조합형 인코딩
- 초성을 위한 비트열, 중성을 위한 비트열, 종성 ..
- 초성, 중성, 종성에 해당하는 코드를 합하여 하나의 글자코드를 만드는 인코딩 방식
- 

